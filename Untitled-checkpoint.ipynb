{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a392849e-6d77-4c62-a563-ce1b9aa0dd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extraction completed using PyMuPDF.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_pymupdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF using PyMuPDF while preserving structure.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    extracted_text = \"\"\n",
    "\n",
    "    for page in doc:\n",
    "        extracted_text += page.get_text(\"text\") + \"\\n\"  # Extracts structured text\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "# Example usage\n",
    "pdf_files = [\"physiology.pdf\", \"pathology.pdf\", \"pharmacology.pdf\"]\n",
    "extracted_texts = {pdf: extract_text_pymupdf(pdf) for pdf in pdf_files}\n",
    "\n",
    "# Save extracted content as text files\n",
    "for pdf, text in extracted_texts.items():\n",
    "    with open(f\"{pdf}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "print(\"Text extraction completed using PyMuPDF.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12162749-1675-4123-a015-3ea1628b02d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchy saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import json  # Import json for saving the hierarchy\n",
    "\n",
    "def create_hierarchy(text):\n",
    "    \"\"\"Converts textbook text into a hierarchical structure.\"\"\"\n",
    "    chapters = text.split(\"\\n\\nChapter \")  # Assuming \"Chapter\" marks sections\n",
    "    tree = {\"root\": []}\n",
    "\n",
    "    for i, chapter in enumerate(chapters):\n",
    "        sections = chapter.split(\"\\n\\nSection \")  # Assuming \"Section\" marks subsections\n",
    "        chapter_node = {\"id\": f\"chapter_{i}\", \"sections\": []}\n",
    "\n",
    "        for j, section in enumerate(sections):\n",
    "            paragraphs = section.split(\"\\n\\n\")  # Splitting paragraphs\n",
    "            section_node = {\"id\": f\"section_{i}_{j}\", \"content\": paragraphs}\n",
    "            chapter_node[\"sections\"].append(section_node)\n",
    "\n",
    "        tree[\"root\"].append(chapter_node)\n",
    "\n",
    "    return tree\n",
    "\n",
    "# Ensure 'extracted_texts' contains the processed textbook content\n",
    "extracted_texts = {\"physiology.pdf\": \"Chapter 1\\n\\nSection 1.1\\n\\nParagraph 1...\\n\\nSection 1.2\\n\\nParagraph 2...\"} \n",
    "\n",
    "# Create hierarchy\n",
    "hierarchy = create_hierarchy(extracted_texts[\"physiology.pdf\"])\n",
    "\n",
    "# Save the hierarchy to a JSON file\n",
    "with open(\"hierarchy.json\", \"w\") as f:\n",
    "    json.dump(hierarchy, f, indent=2)\n",
    "\n",
    "print(\"Hierarchy saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25fccb55-ccce-4629-91ac-e6881d40c69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Chapter 1', 0.0), ('1.1 Paragraph 1...', 0.0), ('1.2 Paragraph 2...', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the corpus\n",
    "corpus = [\" \".join(sec[\"content\"]) for ch in hierarchy[\"root\"] for sec in ch[\"sections\"]]\n",
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "\n",
    "# Train BM25 model\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def search_bm25(query):\n",
    "    query_tokens = word_tokenize(query.lower())\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    return sorted(zip(corpus, scores), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Example search for a Physiology-related question\n",
    "print(search_bm25(\"Explain the mechanism of muscle contraction\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b6653a3-d4fd-4050-83be-9cc551429c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chapter 1', '1.2 Paragraph 2...', '1.1 Paragraph 1...', '1.2 Paragraph 2...', '1.2 Paragraph 2...']\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load Sentence-BERT model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(corpus)\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "def search_dense(query):\n",
    "    query_embedding = model.encode([query])\n",
    "    _, indices = index.search(np.array(query_embedding), 5)\n",
    "    return [corpus[i] for i in indices[0]]\n",
    "\n",
    "# Example search\n",
    "print(search_dense(\"diabetes management\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9932c159-583a-4886-b6fe-6b602df37c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37dce4109b224a4999def6de7797122e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUJAL SINHA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SUJAL SINHA\\.cache\\huggingface\\hub\\models--facebook--opt-350m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e74692dd0b46c68df2a870272ef115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df6bc121bf9453d984f93fd2f5a5c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9553798bc542ec8aed3f197d98d6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0256e663b3314dbc8ae38a437469c988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db1966a0d7d4ee8b28ca9183144b114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6e1b4a131145c88ca9d101b7671ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the symptoms of pneumonia?\n",
      "Answer: This is a long-term care facility. The residents are generally well, but they are still getting treated for pneumonia.\n",
      "Q: What does pneumonia mean?\n",
      "Answer: It is\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0161a6a6524a11b032253b52432082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/662M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Use a very small model for better performance on CPU\n",
    "model_name = \"facebook/opt-350m\"\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model with lower precision for efficiency\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\" if device == \"cuda\" else \"cpu\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ").to(device)\n",
    "\n",
    "# Function to generate answers\n",
    "def generate_answer_local(query):\n",
    "    input_text = f\"Question: {query}\\nAnswer:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs, \n",
    "        max_length=50,  # Reduce length to avoid infinite loops\n",
    "        temperature=0.7,  \n",
    "        top_p=0.9,  \n",
    "        do_sample=True\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "print(generate_answer_local(\"What are the symptoms of pneumonia?\"))\n",
    "\n",
    "# Clear CUDA memory (optional)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6ec94c2-5ed3-4824-9411-1fec8351afb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: corpus.txt not found! Creating a sample corpus...\n",
      "Sample corpus.txt created.\n",
      "FAISS index saved as 'medical_index.faiss'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load Sentence-BERT model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ✅ Ensure corpus is loaded\n",
    "corpus_file = \"corpus.txt\"\n",
    "if not os.path.exists(corpus_file):\n",
    "    print(\"Error: corpus.txt not found! Creating a sample corpus...\")\n",
    "    corpus = [\"Diabetes management strategies\", \"Symptoms of pneumonia\", \"Treatment for hypertension\"]\n",
    "    \n",
    "    with open(corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(corpus))\n",
    "    print(\"Sample corpus.txt created.\")\n",
    "else:\n",
    "    with open(corpus_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        corpus = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# ✅ Now encode corpus\n",
    "embeddings = model.encode(corpus)\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# ✅ Save FAISS index\n",
    "faiss.write_index(index, \"medical_index.faiss\")\n",
    "print(\"FAISS index saved as 'medical_index.faiss'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c974c099-3106-4e9b-836e-918c79197ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Diabetes management strategies', 'Treatment for hypertension', 'Symptoms of pneumonia', 'Treatment for hypertension', 'Treatment for hypertension']\n"
     ]
    }
   ],
   "source": [
    "# ✅ Load FAISS index\n",
    "index = faiss.read_index(\"medical_index.faiss\")\n",
    "\n",
    "def search_dense(query):\n",
    "    query_embedding = model.encode([query])\n",
    "    _, indices = index.search(np.array(query_embedding), 5)\n",
    "    return [corpus[i] for i in indices[0]]\n",
    "\n",
    "# ✅ Example search\n",
    "print(search_dense(\"diabetes management\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "952abdf5-09bf-492b-970a-a04fd14fcd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Symptoms of pneumonia\n",
      " Treatment for hypertension Diabetes management strategies\n",
      "\n",
      "\n",
      "Question: What are the symptoms of pneumonia?\n",
      "Answer:\n",
      "\n",
      "Symptoms of pneumonia can range from mild to severe. Some symptoms may include a fever, difficulty breathing, loss of appetite, or a loss of sense of taste or smell. Symptoms of pneumonia are most commonly associated with a cough, fever, and loss of appetite.\n",
      "\n",
      "Symptoms of pneumonia may not appear until after you are in the hospital. Some\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Use a lightweight model\n",
    "model_name = \"facebook/opt-350m\"\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\" if device == \"cuda\" else \"cpu\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ").to(device)\n",
    "\n",
    "# Load sentence transformer for embeddings\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load FAISS index (assuming `index` and `corpus` are already built)\n",
    "index = faiss.read_index(\"medical_index.faiss\")  # Load FAISS index\n",
    "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = f.readlines()  # Load the text data\n",
    "\n",
    "# Search FAISS for relevant context\n",
    "def search_dense(query):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    _, indices = index.search(np.array(query_embedding), 3)  # Get top 3 docs\n",
    "    return [corpus[i] for i in indices[0]]\n",
    "\n",
    "# Function to generate answers using retrieved context\n",
    "def generate_answer_local(query):\n",
    "    retrieved_docs = search_dense(query)  # Get relevant docs\n",
    "    context = \" \".join(retrieved_docs)  # Merge context\n",
    "\n",
    "    # Improve prompt formatting\n",
    "    input_text = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs, \n",
    "        max_length=100,  \n",
    "        temperature=0.7,  \n",
    "        top_p=0.9,  \n",
    "        do_sample=True\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "print(generate_answer_local(\"What are the symptoms of pneumonia?\"))\n",
    "\n",
    "# Clear CUDA memory (optional)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767917ed-9210-4599-8f9e-2a118c9cd3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
