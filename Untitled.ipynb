{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a392849e-6d77-4c62-a563-ce1b9aa0dd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extraction completed using PyMuPDF.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_pymupdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF using PyMuPDF while preserving structure.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    extracted_text = \"\"\n",
    "\n",
    "    for page in doc:\n",
    "        extracted_text += page.get_text(\"text\") + \"\\n\"  # Extracts structured text\n",
    "\n",
    "    return extracted_text\n",
    "\n",
    "# Example usage\n",
    "pdf_files = [\"physiology.pdf\", \"pathology.pdf\", \"pharmacology.pdf\"]\n",
    "extracted_texts = {pdf: extract_text_pymupdf(pdf) for pdf in pdf_files}\n",
    "\n",
    "# Save extracted content as text files\n",
    "for pdf, text in extracted_texts.items():\n",
    "    with open(f\"{pdf}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "print(\"Text extraction completed using PyMuPDF.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12162749-1675-4123-a015-3ea1628b02d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchy saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import json  # Import json for saving the hierarchy\n",
    "\n",
    "def create_hierarchy(text):\n",
    "    \"\"\"Converts textbook text into a hierarchical structure.\"\"\"\n",
    "    chapters = text.split(\"\\n\\nChapter \")  # Assuming \"Chapter\" marks sections\n",
    "    tree = {\"root\": []}\n",
    "\n",
    "    for i, chapter in enumerate(chapters):\n",
    "        sections = chapter.split(\"\\n\\nSection \")  # Assuming \"Section\" marks subsections\n",
    "        chapter_node = {\"id\": f\"chapter_{i}\", \"sections\": []}\n",
    "\n",
    "        for j, section in enumerate(sections):\n",
    "            paragraphs = section.split(\"\\n\\n\")  # Splitting paragraphs\n",
    "            section_node = {\"id\": f\"section_{i}_{j}\", \"content\": paragraphs}\n",
    "            chapter_node[\"sections\"].append(section_node)\n",
    "\n",
    "        tree[\"root\"].append(chapter_node)\n",
    "\n",
    "    return tree\n",
    "\n",
    "# Ensure 'extracted_texts' contains the processed textbook content\n",
    "extracted_texts = {\"physiology.pdf\": \"Chapter 1\\n\\nSection 1.1\\n\\nParagraph 1...\\n\\nSection 1.2\\n\\nParagraph 2...\"} \n",
    "\n",
    "# Create hierarchy\n",
    "hierarchy = create_hierarchy(extracted_texts[\"physiology.pdf\"])\n",
    "\n",
    "# Save the hierarchy to a JSON file\n",
    "with open(\"hierarchy.json\", \"w\") as f:\n",
    "    json.dump(hierarchy, f, indent=2)\n",
    "\n",
    "print(\"Hierarchy saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25fccb55-ccce-4629-91ac-e6881d40c69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Chapter 1', 0.0), ('1.1 Paragraph 1...', 0.0), ('1.2 Paragraph 2...', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenize the corpus\n",
    "corpus = [\" \".join(sec[\"content\"]) for ch in hierarchy[\"root\"] for sec in ch[\"sections\"]]\n",
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "\n",
    "# Train BM25 model\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def search_bm25(query):\n",
    "    query_tokens = word_tokenize(query.lower())\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    return sorted(zip(corpus, scores), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "# Example search for a Physiology-related question\n",
    "print(search_bm25(\"Explain the mechanism of muscle contraction\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b6653a3-d4fd-4050-83be-9cc551429c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chapter 1', '1.2 Paragraph 2...', '1.1 Paragraph 1...', '1.2 Paragraph 2...', '1.2 Paragraph 2...']\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load Sentence-BERT model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(corpus)\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "def search_dense(query):\n",
    "    query_embedding = model.encode([query])\n",
    "    _, indices = index.search(np.array(query_embedding), 5)\n",
    "    return [corpus[i] for i in indices[0]]\n",
    "\n",
    "# Example search\n",
    "print(search_dense(\"diabetes management\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9932c159-583a-4886-b6fe-6b602df37c70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37dce4109b224a4999def6de7797122e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUJAL SINHA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SUJAL SINHA\\.cache\\huggingface\\hub\\models--facebook--opt-350m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e74692dd0b46c68df2a870272ef115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df6bc121bf9453d984f93fd2f5a5c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9553798bc542ec8aed3f197d98d6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0256e663b3314dbc8ae38a437469c988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db1966a0d7d4ee8b28ca9183144b114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/663M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be6e1b4a131145c88ca9d101b7671ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the symptoms of pneumonia?\n",
      "Answer: This is a long-term care facility. The residents are generally well, but they are still getting treated for pneumonia.\n",
      "Q: What does pneumonia mean?\n",
      "Answer: It is\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc0161a6a6524a11b032253b52432082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/662M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Use a very small model for better performance on CPU\n",
    "model_name = \"facebook/opt-350m\"\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load model with lower precision for efficiency\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\" if device == \"cuda\" else \"cpu\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ").to(device)\n",
    "\n",
    "# Function to generate answers\n",
    "def generate_answer_local(query):\n",
    "    input_text = f\"Question: {query}\\nAnswer:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs, \n",
    "        max_length=50,  # Reduce length to avoid infinite loops\n",
    "        temperature=0.7,  \n",
    "        top_p=0.9,  \n",
    "        do_sample=True\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "print(generate_answer_local(\"What are the symptoms of pneumonia?\"))\n",
    "\n",
    "# Clear CUDA memory (optional)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6ec94c2-5ed3-4824-9411-1fec8351afb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved as 'medical_index.faiss'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Load Sentence-BERT model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# ✅ Ensure corpus is loaded\n",
    "corpus_file = \"corpus.txt\"\n",
    "if not os.path.exists(corpus_file):\n",
    "    print(\"Error: corpus.txt not found! Creating a sample corpus...\")\n",
    "    corpus = [\"Diabetes management strategies\", \"Symptoms of pneumonia\", \"Treatment for hypertension\"]\n",
    "    \n",
    "    with open(corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(corpus))\n",
    "    print(\"Sample corpus.txt created.\")\n",
    "else:\n",
    "    with open(corpus_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        corpus = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# ✅ Now encode corpus\n",
    "embeddings = model.encode(corpus)\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# ✅ Save FAISS index\n",
    "faiss.write_index(index, \"medical_index.faiss\")\n",
    "print(\"FAISS index saved as 'medical_index.faiss'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c974c099-3106-4e9b-836e-918c79197ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Load FAISS index\n",
    "# index = faiss.read_index(\"medical_index.faiss\")\n",
    "\n",
    "# def search_dense(query):\n",
    "#     query_embedding = model.encode([query])\n",
    "#     _, indices = index.search(np.array(query_embedding), 5)\n",
    "#     return [corpus[i] for i in indices[0]]\n",
    "\n",
    "# # ✅ Example search\n",
    "# print(search_dense(\"diabetes management\"))\n",
    "# Ensure corpus is clean and normalized\n",
    "corpus = [line.strip().lower() for line in open(\"corpus.txt\", \"r\", encoding=\"utf-8\").readlines()]\n",
    "\n",
    "# Encode corpus\n",
    "embeddings = model.encode(corpus, convert_to_numpy=True)\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "952abdf5-09bf-492b-970a-a04fd14fcd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: Symptoms of pneumonia\n",
      " Treatment for hypertension Diabetes management strategies\n",
      "\n",
      "\n",
      "Question: What are the symptoms of pneumonia?\n",
      "Answer:\n",
      "\n",
      "Symptoms of pneumonia can range from mild to severe. Some symptoms may include a fever, difficulty breathing, loss of appetite, or a loss of sense of taste or smell. Symptoms of pneumonia are most commonly associated with a cough, fever, and loss of appetite.\n",
      "\n",
      "Symptoms of pneumonia may not appear until after you are in the hospital. Some\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Use a lightweight model\n",
    "model_name = \"facebook/opt-350m\"\n",
    "\n",
    "# Check if GPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load tokenizer & model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\" if device == \"cuda\" else \"cpu\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ").to(device)\n",
    "\n",
    "# Load sentence transformer for embeddings\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Load FAISS index (assuming `index` and `corpus` are already built)\n",
    "index = faiss.read_index(\"medical_index.faiss\")  # Load FAISS index\n",
    "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = f.readlines()  # Load the text data\n",
    "\n",
    "# Search FAISS for relevant context\n",
    "def search_dense(query):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    _, indices = index.search(np.array(query_embedding), 3)  # Get top 3 docs\n",
    "    return [corpus[i] for i in indices[0]]\n",
    "\n",
    "# Function to generate answers using retrieved context\n",
    "def generate_answer_local(query):\n",
    "    retrieved_docs = search_dense(query)  # Get relevant docs\n",
    "    context = \" \".join(retrieved_docs)  # Merge context\n",
    "\n",
    "    # Improve prompt formatting\n",
    "    input_text = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    output = model.generate(\n",
    "        **inputs, \n",
    "        max_length=100,  \n",
    "        temperature=0.7,  \n",
    "        top_p=0.9,  \n",
    "        do_sample=True\n",
    "    )\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "print(generate_answer_local(\"What are the symptoms of pneumonia?\"))\n",
    "\n",
    "# Clear CUDA memory (optional)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2337d23-9c51-401d-8306-52057e4ed0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Corpus updated with real medical content.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = []\n",
    "    for page in doc:\n",
    "        text.append(page.get_text(\"text\"))\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "# Extract from all PDFs\n",
    "pdf_paths = [\"pathology.pdf\", \"physiology.pdf\", \"pharmacology.pdf\"]\n",
    "full_corpus = []\n",
    "for pdf in pdf_paths:\n",
    "    full_corpus.append(extract_text_from_pdf(pdf))\n",
    "\n",
    "# Save extracted text\n",
    "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(full_corpus))\n",
    "\n",
    "print(\"✅ Corpus updated with real medical content.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1e369cb-17ad-41c5-bb09-25ff8fbbdf4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📚 Corpus now contains 118499 entries.\n",
      "✅ FAISS index rebuilt successfully.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# ✅ Load updated corpus\n",
    "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = [line.strip().lower() for line in f.readlines() if line.strip()]\n",
    "\n",
    "# ✅ Check corpus size\n",
    "print(f\"📚 Corpus now contains {len(corpus)} entries.\")\n",
    "\n",
    "# ✅ Encode full-text corpus\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embeddings = embedding_model.encode(corpus, convert_to_numpy=True)\n",
    "\n",
    "# ✅ Rebuild FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"✅ FAISS index rebuilt successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e8b8dc5-490a-49d9-bef7-547fb17f036a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['developing pneumonia.', 'pneumonias', 'pneumonias', 'bacterial pneumonias.', 'pneumonia']\n",
      "['diabetes', 'diabetes', 'diet, exercise, oral drugs, insulin', 'require insulin therapy to control hyperglycaemia or to', 'chronic debilitating conditions like uncontrolled diabetes,']\n",
      "['drugs for the combined therapy of hypertension are selected to minimize', 'severe hypertension must be treated with the combination of drugs', 'orally for the treatment of mild to moderate chronic hypertension.', 'drugs for hypertension emergency', 'with systemic hypertension has been suggested by some']\n"
     ]
    }
   ],
   "source": [
    "print(search_dense(\"What are the symptoms of pneumonia?\"))\n",
    "print(search_dense(\"How to manage diabetes?\"))\n",
    "print(search_dense(\"Best treatment for hypertension?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85c134f2-0cff-4cde-b96b-218032e802b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Search Results: ['pneumonia', 'of gestation. symptoms of cytomegalovirus infection are', 'symptoms', 'bacterial pneumonias.', 'developing pneumonia.']\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# ✅ Load Sentence-BERT model for dense embeddings\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ✅ Load Corpus\n",
    "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = [line.strip().lower() for line in f.readlines()]\n",
    "\n",
    "# ✅ Encode corpus with Sentence-BERT\n",
    "embeddings = embedding_model.encode(corpus, convert_to_numpy=True)\n",
    "\n",
    "# ✅ Build FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "# ✅ Tokenize corpus for BM25\n",
    "tokenized_corpus = [doc.split() for doc in corpus]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# ✅ FAISS Semantic Search\n",
    "def search_dense(query, top_k=5, min_score=0.5):\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        if distances[0][i] > min_score:  # ✅ Filter low-score matches\n",
    "            results.append(corpus[idx])\n",
    "\n",
    "    return results if results else [\"No relevant context found.\"]\n",
    "\n",
    "# ✅ BM25 Keyword-Based Search\n",
    "def search_bm25(query, top_k=5):\n",
    "    tokenized_query = query.split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_n = np.argsort(scores)[::-1][:top_k]\n",
    "    return [corpus[i] for i in top_n]\n",
    "\n",
    "# ✅ Hybrid Search: FAISS + BM25 Reranking\n",
    "def hybrid_search(query, top_k=5):\n",
    "    faiss_results = search_dense(query, top_k)\n",
    "    bm25_results = search_bm25(query, top_k)\n",
    "\n",
    "    # ✅ Merge & Deduplicate Results\n",
    "    final_results = list(set(faiss_results + bm25_results))[:top_k]\n",
    "    return final_results\n",
    "\n",
    "# ✅ Example Usage\n",
    "query = \"What are the symptoms of pneumonia?\"\n",
    "results = hybrid_search(query)\n",
    "print(\"🔍 Search Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "21e597ce-22df-4cfd-80e4-0182879425bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e1cb83ba2a44abb8aa0127b17b6f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/685 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SUJAL SINHA\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\SUJAL SINHA\\.cache\\huggingface\\hub\\models--facebook--opt-125m. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2130dd129857441a87ec07116e7d6547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/651 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274d13ff2aff461da6d0ca27cc7737e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49318925bd684524845601032091c867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e57ad342d24a16ae2d2198d7233189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/441 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cfd1c0f7fb347b68fd2e8ff8dbb3b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/251M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1f59ade51724c308e88796762484208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: pneumonias pneumonias developing pneumonia.\n",
      "Question: What are the symptoms of pneumonia?\n",
      "Answer:\n",
      "I have been told that I have pneumonia and my lungs are starting to dry up.\n",
      "I have been told that I\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# ✅ Use a smaller & faster model\n",
    "model_name = \"facebook/opt-125m\"  # Switch to \"mistral-7B-instruct-v0.1\" if GPU available\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model efficiently\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16 if device == \"cuda\" else torch.float32).to(device)\n",
    "\n",
    "def generate_answer(query):\n",
    "    # Retrieve relevant contexts\n",
    "    context = search_dense(query, top_k=3)  # Reduce retrieved passages\n",
    "    context_text = \" \".join(context)\n",
    "\n",
    "    # Format input for LLM\n",
    "    input_text = f\"Context: {context_text}\\nQuestion: {query}\\nAnswer:\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # ✅ Use a lower max_length & increase top_k/top_p for speed\n",
    "    output = model.generate(\n",
    "        **inputs, \n",
    "        max_length=50,  # Reduce token count\n",
    "        temperature=0.7,  \n",
    "        top_p=0.85,  \n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Example\n",
    "print(generate_answer(\"What are the symptoms of pneumonia?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bafb8fc4-0485-4100-8121-bc95fb640656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pneumonias\n",
      "diabetes\n",
      "combination of drugs orally\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ✅ Load Sentence-BERT for retrieval\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ✅ FAISS index should be already built & loaded\n",
    "# Ensure `corpus` and `index` exist in your environment\n",
    "\n",
    "def search_dense(query, top_k=3):\n",
    "    \"\"\"Retrieve relevant contexts using FAISS.\"\"\"\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # ✅ Fix: Avoid out-of-bounds error\n",
    "    results = [\n",
    "        corpus[idx] for i, idx in enumerate(indices[0]) \n",
    "        if idx < len(corpus) and distances[0][i] < 0.8  # Valid index + threshold\n",
    "    ]\n",
    "\n",
    "    return results if results else [\"No relevant context found.\"]\n",
    "\n",
    "# ✅ Load lightweight FLAN-T5 for generation\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "def generate_answer(query):\n",
    "    \"\"\"Generate answers using FLAN-T5 with retrieved context.\"\"\"\n",
    "    context = search_dense(query, top_k=3)  \n",
    "    context_text = \" \".join(context)\n",
    "\n",
    "    # Format input for factual QA\n",
    "    input_text = f\"\"\"\n",
    "    You are a medical assistant answering user queries based on the given context.\n",
    "\n",
    "    Context:\n",
    "    {context_text}\n",
    "\n",
    "    Question: {query}\n",
    "    Provide a short and precise answer.\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize and generate response\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    output = model.generate(\n",
    "        **inputs, \n",
    "        max_length=50,  \n",
    "        temperature=0.7,  \n",
    "        top_p=0.85,  \n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# ✅ Example Queries\n",
    "print(generate_answer(\"What are the symptoms of pneumonia?\"))\n",
    "print(generate_answer(\"How to manage diabetes?\"))\n",
    "print(generate_answer(\"Best treatment for hypertension?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1c88598-9e0c-443a-9b29-cdca187de5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I couldn't find relevant information.\n",
      "mellitus\n",
      "combination of drugs orally\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "\n",
    "# ✅ Load sentence embedding model (FAISS-compatible)\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# ✅ Load corpus (Ensure it has meaningful sentences)\n",
    "with open(\"corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    corpus = [line.strip().lower() for line in f.readlines()]\n",
    "\n",
    "# ✅ Encode corpus & Build FAISS Index\n",
    "embeddings = embedding_model.encode(corpus, convert_to_numpy=True)\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "# ✅ FAISS Search Function with Improved Filtering\n",
    "def search_dense(query, top_k=3):\n",
    "    \"\"\"Retrieve relevant contexts using FAISS with meaningful text filtering.\"\"\"\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # ✅ Filter out short, irrelevant text\n",
    "    results = [\n",
    "        corpus[idx] for i, idx in enumerate(indices[0]) \n",
    "        if idx < len(corpus) and distances[0][i] < 0.8 and len(corpus[idx].split()) > 5\n",
    "    ]\n",
    "    return results if results else [\"No relevant context found.\"]\n",
    "\n",
    "# ✅ Load lightweight FLAN-T5 model for fast generation\n",
    "model_name = \"google/flan-t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "# ✅ Generate medical answers with FAISS-based retrieval\n",
    "def generate_answer(query):\n",
    "    \"\"\"Generate medical answers using FLAN-T5 with FAISS-retrieved context.\"\"\"\n",
    "    context = search_dense(query, top_k=3)\n",
    "    context_text = \" \".join(context)\n",
    "\n",
    "    if \"No relevant context found.\" in context_text:\n",
    "        return \"I'm sorry, but I couldn't find relevant information.\"\n",
    "\n",
    "    # ✅ Improve prompt structure for better answers\n",
    "    input_text = f\"\"\"\n",
    "    You are a medical assistant. Answer based on the given context.\n",
    "\n",
    "    Context:\n",
    "    {context_text}\n",
    "\n",
    "    Question: {query}\n",
    "    \n",
    "    Provide a short, precise medical answer.\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize & Generate response\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True).to(device)\n",
    "    output = model.generate(**inputs, max_length=100, temperature=0.7, top_p=0.9, do_sample=True)\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# ✅ Test with sample queries\n",
    "print(generate_answer(\"What are the symptoms of pneumonia?\"))\n",
    "print(generate_answer(\"How is diabetes managed?\"))\n",
    "print(generate_answer(\"What is the best treatment for hypertension?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a21749-d130-489a-a1e9-5845861b1f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
